// Trial pipeline
// Needs cutdapt, samtools, bwa, picard, java, varscan2, fastqc, bedtools.
// Annovar script table_annovar.pl is a customised version of that provided with annovar, needs to replace the default version.
//      i.e. you need to install annovar and then replace table_annovar.pl with the copy found with this pipeline
// Report generation function to be added (soon)

// input is 4 *.fastq.gz files. 2 sets of paired end files, one for tumour, one for normal.

bwaIndex="/data/data/index/ucsc.hg19.fasta"
threads="1"

//qc = {
//    var THREADS : 1
//    input.dir="data/fastq"
//    output.dir="data/fastqc/"
//    //exec "module load FastQC"
//    multi "fastqc -t ${THREADS} $input1 -o $output.dir",
//        "fastqc -t ${THREADS} $input2 -o $output.dir",
//        "fastqc -t ${THREADS} $input3 -o $output.dir",
//        "fastqc -t ${THREADS} $input4 -o $output.dir"
//}


fastqc_docker = {
fastqc_out="/data/data/fastqc"
	// Path must be PATH INSIDE Docker Container /data directory
    //transform('.fastq.gz') to('_fastqc.zip') {

    multi "docker run --rm -v ${PWD}:/data -w /data alexcoppe/fastqc -o ${fastqc_out} --noextract $input1",
          "docker run --rm -v ${PWD}:/data -w /data alexcoppe/fastqc -o ${fastqc_out} --noextract $input2",
          "docker run --rm -v ${PWD}:/data -w /data alexcoppe/fastqc -o ${fastqc_out} --noextract $input3",
          "docker run --rm -v ${PWD}:/data -w /data alexcoppe/fastqc -o ${fastqc_out} --noextract $input4"

   
}


trim = {
    output.dir="data/intermediateFiles"
    // this assumes that the input is illumina, as we're not autodetecting across a variety of possible adapters,
    // which would be possible with trim_glaore, but that requires changes to trim_galores output methods.
    multi "source ./config && docker run --rm -v ${PWD}:/data -w /data jduckles/cutadapt -a ${ADAPTER} -q ${QUALITY_CUTOFF} --minimum-length=${MIN_LENGTH} -o $output1 -p $output2 $input1 $input2",
        "source ./config && docker run --rm -v ${PWD}:/data -w /data jduckles/cutadapt -a ${ADAPTER} -q ${QUALITY_CUTOFF} --minimum-length=${MIN_LENGTH} -o $output3 -p $output4 $input3 $input4"
}



align = {
    output.dir="data/intermediateFiles"
    var SAMPLE : "SampleThis"

    // -R '@RG\tID:${SAMPLE}_02\tSM:$SAMPLE\tLB:${SAMPLE}_02\tPL:ILLUMINA' 
    multi """	
	docker run -v ${PWD}:/data -w /data gacancergenomics/bwa /bin/bash -c \"bwa mem -t 1 $bwaIndex $input1 $input2 > $output1.sam\"; 
docker run -v ${PWD}:/data -w /data gacancergenomics/samtools samtools sort -@1 -O BAM -o $output1.bam $output1.sam
""", """
docker run -v ${PWD}:/data -w /data gacancergenomics/bwa /bin/bash -c \"bwa mem -t 1 $bwaIndex $input3 $input4 > $output2.sam\";
docker run -v ${PWD}:/data -w /data gacancergenomics/samtools samtools sort -@1 -O BAM -o $output2.bam $output2.sam
"""
}

removeDuplicates = {
    output.dir="data/intermediateFiles"
    multi "docker run -v ${PWD}:/data/ -w /data gacancergenomics/picard picard MarkDuplicates I=$input1.bam O=$output1.bam M=$output._dup_metrics.txt REMOVE_DUPLICATES=TRUE CREATE_INDEX=TRUE",
        "docker run -v ${PWD}:/data/ -w /data gacancergenomics/picard picard MarkDuplicates I=$input2.bam O=$output2.bam M=$output._dup_metrics.txt REMOVE_DUPLICATES=TRUE CREATE_INDEX=TRUE"

}



removeSupplementary = {
    output.dir="data/intermediateFiles"
    multi """docker run -v ${PWD}:/data/ -w /data gacancergenomics/samtools /bin/bash -c "samtools view -@ 4 -b -F 2048 $input1.bam > $output1.bam" 
""",
        """ docker run -v ${PWD}:/data/ -w /data gacancergenomics/samtools /bin/bash -c "samtools view -@ 4 -b -F 2048 $input2.bam > $output2.bam" """ 
    multi """ docker run -v ${PWD}:/data/ -w /data gacancergenomics/samtools /bin/bash -c "samtools index $output1.bam" """,
        """ docker run -v ${PWD}:/data/ -w /data gacancergenomics/samtools /bin/bash -c "samtools index $output2.bam" """

}

pileUp = {
    output.dir="data/intermediateFiles"
    //exec "samtools mpileup -B -d 9000 -q 1 -f $bwaIndex -o $output $input1.bam $input2.bam"
    exec """ docker run -v ${PWD}:/data/ -w /data gacancergenomics/samtools /bin/bash -c "samtools mpileup -B -d 9000 -q 1 -f $bwaIndex  $input1.bam $input2.bam > $output1.bam " """

//,  java -Xmx16g -d64 -jar /usr/local/jar/VarScan.v2.4.3.jar somatic --mpileup 1 --min-var-freq 0.1 --p-value 1.00 --somatic-p-value 1.00 --strand-filter 0 --tumor-purity 0.5 --output-vcf 1 --min-coverage-normal 10 --min-coverage-tumor 10 --output-snp $output1.vcf  --output-indel $output2.vcf"
}

variantCalling = {
   output.dir="data/intermediateFiles"

    // it should be noted that output1 is snps, output2 is indels
    VARSCAN="/nesi/project/uoa00571/software/VarScan.v2.4.3.jar"
    exec "java -Xmx8g -jar /nesi/project/uoa00571/software/VarScan.v2.4.3.jar somatic $input --mpileup 1 --min-var-freq 0.1 --p-value 1.00 --somatic-p-value 1.00 --strand-filter 0 --tumor-purity 0.5 --output-vcf 1 --min-coverage-normal 10 --min-coverage-tumor 10 --output-snp $output1.vcf  --output-indel $output2.vcf"
}

annotation = {
    output.dir="data/intermediateFiles"

    // load mahuika modules - the bpipe config method of doing this doesn't appear to be working.
    //exec "module load VCFtools"
    //exec "module load SAMtools"
    //exec "module load BEDTools/2.26.0-gimkl-2017a"
    //exec "module load Perl"


    // Version 2 of our annotation
	multi "perl src/table_annovar.pl $input hg19  -buildver hg19 -outfile $output.vcf -remove -protocol refGene,popfreq_max_20150413,dbnsfp30a,tfbsConsSites,targetScanS,genomicSuperDups,clinvar_20170130,cosmic82,avsnp147,rmsk,wgEncodeDacMapabilityConsensusExcludable,wgEncodeDukeMapabilityRegionsExcludable,gnomad_genome,gnomad_exome,exac03,intervar_20170202,revel,dgvMerged  -operation g,f,f,r,r,r,f,f,f,r,r,r,f,f,f,f,f,r  -nastring . -vcfinput",
    	      "perl src/table_annovar.pl $input2 hg19  -buildver hg19 -outfile $output2.vcf -remove -protocol refGene,popfreq_max_20150413,dbnsfp30a,tfbsConsSites,targetScanS,genomicSuperDups,clinvar_20170130,cosmic82,avsnp147,rmsk,wgEncodeDacMapabilityConsensusExcludable,wgEncodeDukeMapabilityRegionsExcludable,gnomad_genome,gnomad_exome,exac03,intervar_20170202,revel,dgvMerged  -operation g,f,f,r,r,r,f,f,f,r,r,r,f,f,f,f,f,r  -nastring . -vcfinput"

}

count = {
  output.dir="data/intermediateFiles"
  exec "cp $input1.vcf temp1.vcf"
  zipped = "$input1" + ".gz"
  exec "bgzip -f $input1.vcf"
  exec "tabix -f -p vcf $zipped"
  exec "cp temp1.vcf $input1.vcf"


  exec "cp $input2.vcf temp2.vcf"
  exec "bgzip -f $input2.vcf"
  zipped2 = "$input2" + ".gz"
  exec "tabix -f -p vcf $zipped2"
  exec "cp temp2.vcf $input2.vcf"


  //This really doesn't fall over gracefully when the filters are to strict.
  //Also, this is appallingly bad form. The only difference between the two calls are a bit flag and the presence/absence of some filters.
  //it should be a single function called twice with different parameters.
  exec "Rscript src/filePairs.R $zipped $zipped2 $output"
  exec "Rscript src/filePairsSomatic.R $zipped $zipped2 $output2"

}

reportGeneration ={
	output.dir="results"
   exec "Rscript src/generateReport.R $input1 $input2 $output1"
}

cleanup_bams = {
        cleanup "*.bam", "*.bam"
}


// currently runs qc, doeen't wait for confirmation on whether to continue the run,
// trims, and does a 2nd lot of qc to the trimmed reads.
// Once this is parrallelized, should be able to insert multiqc steps.

// run with qc
// run { [qc + trim] + [qc + align] + removeDuplicates + removeSuppplementary + pileUp + variantCalling + annotation}

// run without qc
//run {  align +  removeDuplicates + removeSuppplementary  + pileUp + annotation + count + reportGeneration}

//run { fastqc_docker }
run { fastqc_docker  + trim  + align + removeDuplicates + removeSupplementary + pileUp}
